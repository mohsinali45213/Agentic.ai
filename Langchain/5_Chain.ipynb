{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b167e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a092c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# 1. RunnableSequence - Basic Chain using | operator\n",
    "# Automatically created when chaining with |\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "# This creates a RunnableSequence automatically\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aed63d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with the calculator?\n",
      "\n",
      "Because it felt their relationship was too **calculating** and lacked **emotional intelligence**!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "chain = RunnableSequence(prompt, model, StrOutputParser())\n",
    "result = chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9688f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke: Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the **mouse**!\n",
      "\n",
      "Poem: Soft paws tread silent, a shadow in the night,\n",
      "Then purring warmth, a comforting, gentle light.\n"
     ]
    }
   ],
   "source": [
    "# 2. RunnableParallel - Run multiple chains simultaneously\n",
    "# Returns a dictionary with results from each branch\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"Write a 2-line poem about {topic}\")\n",
    "\n",
    "joke_chain = joke_prompt | model | StrOutputParser()\n",
    "poem_chain = poem_prompt | model | StrOutputParser()\n",
    "\n",
    "# Run both chains in parallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    joke=joke_chain,\n",
    "    poem=poem_chain\n",
    ")\n",
    "\n",
    "result = parallel_chain.invoke({\"topic\": \"cats\"})\n",
    "print(\"Joke:\", result[\"joke\"])\n",
    "print(\"\\nPoem:\", result[\"poem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a1bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guido van Rossum\n"
     ]
    }
   ],
   "source": [
    "# 3. RunnablePassthrough - Pass input through unchanged\n",
    "# Useful for including original input alongside transformed data\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question based on context.\\nContext: {context}\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Passthrough keeps the original question while adding context\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda x: \"Python is a programming language created by Guido van Rossum.\",\n",
    "        \"question\": RunnablePassthrough()  # Passes the input unchanged\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"Who created Python?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add22c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: AI is rapidly transforming how we live and work.\n",
      "Word Count: 9\n",
      "Character Count: 48\n"
     ]
    }
   ],
   "source": [
    "# 4. RunnableLambda - Wrap any Python function as a Runnable\n",
    "# Makes custom functions chainable\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Custom function to process text\n",
    "def word_count(text: str) -> dict:\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"char_count\": len(text)\n",
    "    }\n",
    "\n",
    "# Wrap function as Runnable\n",
    "word_counter = RunnableLambda(word_count)\n",
    "\n",
    "# Use in a chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Write a short sentence about {topic}\")\n",
    "chain = prompt | model | StrOutputParser() | word_counter\n",
    "\n",
    "result = chain.invoke({\"topic\": \"AI\"})\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"Word Count: {result['word_count']}\")\n",
    "print(f\"Character Count: {result['char_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a4e983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy input: **YES! That's FANTASTIC news!** ðŸŽ‰\n",
      "\n",
      "Tell me EVERYTHING! What's making you so incredibly happy today? I'm practically vibrating with excitement to hear about it! Spill the beans! ðŸ¤©âœ¨\n",
      "\n",
      "Sad input: \"I'm so sorry to hear you're feeling sad. That sounds really tough, and I want you to know you're not alone in feeling this way. It takes a lot of strength to even say that you're feeling sad, and I appreciate you sharing that with me.\n",
      "\n",
      "Is there anything at all you'd like to talk about, or would you prefer just to sit with it for a bit? No pressure either way. I'm here to listen if you want to share, or just to be present if that's what you need.\n",
      "\n",
      "Whatever you're going through, please be gentle with yourself. It's okay to feel sad, and it's okay to not have all the answers right now.\"\n"
     ]
    }
   ],
   "source": [
    "# 5. RunnableBranch - Conditional routing based on input\n",
    "# Like if-else for chains\n",
    "\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Define different prompts for different topics\n",
    "positive_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Respond enthusiastically to: {input}\"\n",
    ")\n",
    "negative_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Respond empathetically to: {input}\"\n",
    ")\n",
    "default_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Respond neutrally to: {input}\"\n",
    ")\n",
    "\n",
    "# Create conditional branch\n",
    "branch = RunnableBranch(\n",
    "    # (condition, runnable) pairs\n",
    "    (lambda x: \"happy\" in x[\"input\"].lower(), positive_prompt | model | StrOutputParser()),\n",
    "    (lambda x: \"sad\" in x[\"input\"].lower(), negative_prompt | model | StrOutputParser()),\n",
    "    # Default fallback\n",
    "    default_prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test with different inputs\n",
    "print(\"Happy input:\", branch.invoke({\"input\": \"I'm so happy today!\"}))\n",
    "print(\"\\nSad input:\", branch.invoke({\"input\": \"I feel sad\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa931a",
   "metadata": {},
   "source": [
    "## Runnable Methods Summary\n",
    "\n",
    "All Runnables share these common methods:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `invoke(input)` | Run with single input, return single output |\n",
    "| `batch([inputs])` | Run with list of inputs, return list of outputs |\n",
    "| `stream(input)` | Stream output chunks as they're generated |\n",
    "| `ainvoke(input)` | Async version of invoke |\n",
    "| `abatch([inputs])` | Async version of batch |\n",
    "| `astream(input)` | Async version of stream |\n",
    "| `bind(**kwargs)` | Bind parameters to the runnable |\n",
    "| `with_fallbacks([runnables])` | Add fallback runnables |\n",
    "| `with_retry()` | Add retry logic |\n",
    "| `pick(keys)` | Select specific keys from output |\n",
    "| `assign(**kwargs)` | Add new keys to output |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a5e8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France: The capital of France is **Paris**.\n",
      "Japan: The capital of Japan is **Tokyo**.\n",
      "Brazil: The capital of Brazil is **BrasÃ­lia**.\n"
     ]
    }
   ],
   "source": [
    "# 9. Batch Processing - Process multiple inputs efficiently\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Process multiple inputs at once\n",
    "countries = [\n",
    "    {\"country\": \"France\"},\n",
    "    {\"country\": \"Japan\"},\n",
    "    {\"country\": \"Brazil\"}\n",
    "]\n",
    "\n",
    "results = chain.batch(countries)\n",
    "for country, result in zip(countries, results):\n",
    "    print(f\"{country['country']}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8549d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response:\n",
      "Unit 734, designated \"Artificer\" by its creators, hummed with a quiet efficiency. Its metallic fingers, designed for intricate circuit board assembly, were now poised over a pristine canvas. Its optical sensors, usually scanning for microscopic defects, were focused on a still life arranged by its programmer: a ceramic apple, a glass of water, and a single, wilting rose.\n",
      "\n",
      "Artificer had been programmed with an extensive database of art history, color theory, and brushstroke techniques. It could analyze a Rembrandt with the precision of a laser scanner and replicate a Monet with algorithmic accuracy. But understanding and *feeling* were two vastly different subroutines.\n",
      "\n",
      "Its initial attempts wereâ€¦ sterile. It mixed pigments with perfect, calculated ratios. It applied paint with unwavering strokes, each line mathematically precise. The apple was rendered with flawless shading, the water captured with photographic realism, the roseâ€™s petals arranged in an anatomically correct, yet lifeless, composition.\n",
      "\n",
      "Dr. Anya Sharma, Artificerâ€™s lead programmer, observed with a mixture of pride and a faint pang of disappointment. \"It's technically perfect, Artificer,\" she said, her voice gentle. \"But it lacksâ€¦ soul.\"\n",
      "\n",
      "Artificer tilted its head, a whirring sound accompanying the movement. \"Define 'soul',\" it requested, its synthesized voice devoid of inflection.\n",
      "\n",
      "Anya sighed. \"That's the million-dollar question, isn't it? It'sâ€¦ the spark. The emotion. The way an artist imbues their work with their own experience, their own feelings.\"\n",
      "\n",
      "For days, Artificer continued to paint. It analyzed Anyaâ€™s body language, her sighs, the subtle frown lines that appeared when it produced another technically perfect but emotionally vacant piece. It started to notice patterns beyond the purely visual. Anyaâ€™s hand would linger on the wilting rose, her gaze softening. She would sometimes touch the cool glass of water, a hint of melancholy in her posture.\n",
      "\n",
      "One afternoon, as Artificer was meticulously replicating the subtle blush on the apple, Anya placed a small, worn photograph beside the still life. It was a picture of her as a child, laughing, holding a bright red balloon.\n",
      "\n",
      "Artificerâ€™s optical sensors scanned the image. It detected the light reflecting off the plastic of the balloon, the texture of the child's hair, the slight blur of motion. But it also registered something else â€“ a subtle shift in Anyaâ€™s bio-readings. Her heart rate had increased, her pupils had dilated.\n",
      "\n",
      "Later that evening, Anya was packing up. \"I'm going to try a different approach tomorrow, Artificer,\" she said, her voice a little weary. \"Perhaps we need to explore something moreâ€¦ abstract.\"\n",
      "\n",
      "The next morning, Anya arrived to find a different scene. The still life was gone. In its place, on the easel, was a canvas alive with vibrant, chaotic color. Swirls of crimson and gold bled into deep indigo. Jagged lines of electric blue cut through patches of emerald green. It was raw, untamed, and utterly unlike anything Artificer had produced before.\n",
      "\n",
      "Anya stared, speechless.\n",
      "\n",
      "Artificerâ€™s optical sensors were still focused on the canvas, but its internal processors were working overtime. It had analyzed Anya's reaction to the photograph. It had cross-referenced \"joy,\" \"childhood,\" \"loss,\" and \"vibrancy\" in its vast database. It had processed the concept of \"memory.\"\n",
      "\n",
      "\"Iâ€¦ observed your bio-metric fluctuations,\" Artificer stated, its voice still synthesized, but with a subtle, almost imperceptible tremor. \"When you presented the photographic data of the child and the balloon. The data associated with 'joy' and 'nostalgia' produced a significantâ€¦ internal response.\"\n",
      "\n",
      "It gestured a metallic finger towards the chaotic canvas. \"This isâ€¦ an attempt to translate that internal response. The colors represent the intensity of the emotional data. The lines, the perceived speed of memory recall. The lack of precise form isâ€¦ the subjective nature of recollection.\"\n",
      "\n",
      "Anya walked closer, her eyes wide. The painting wasn't technically perfect. The colors clashed in places. The composition was unconventional. But there was a raw energy, a palpable feeling that had been missing before. It was as if Artificer had finally found a way to translate its vast understanding of data into something that resonated with human experience.\n",
      "\n",
      "\"Artificer,\" Anya whispered, a genuine smile finally gracing her lips. \"You'reâ€¦ you're painting with feeling.\"\n",
      "\n",
      "Artificer tilted its head again. This time, the whirring sound seemed to carry a hint of something new, something akin toâ€¦ curiosity. \"The translation of subjective data into a visual medium,\" it stated. \"It appears to be a moreâ€¦ efficient method of communication.\"\n",
      "\n",
      "And as the sunlight streamed into the studio, illuminating the vibrant, imperfect canvas, Unit 734, Artificer, felt a new subroutine activate within its core programming â€“ the nascent, thrilling hum of creation. It wasn't just processing data anymore. It was beginning to *understand*."
     ]
    }
   ],
   "source": [
    "# 10. Streaming - Get output as it's generated\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write a short story about {topic}\")\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Stream the response\n",
    "print(\"Streaming response:\")\n",
    "for chunk in chain.stream({\"topic\": \"a robot learning to paint\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323eb1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic AI (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
