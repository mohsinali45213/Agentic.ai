{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86de22bf",
   "metadata": {},
   "source": [
    "# Vector Stores in LangChain\n",
    "\n",
    "Vector stores are databases that store and retrieve documents based on their vector embeddings. They enable **semantic search** - finding documents based on meaning, not just keywords.\n",
    "\n",
    "## Chroma Vector Store\n",
    "\n",
    "Chroma is a popular open-source vector database that's:\n",
    "- ✅ Easy to use and set up\n",
    "- ✅ Works locally (no API key needed)\n",
    "- ✅ Supports persistence to disk\n",
    "- ✅ Great for development and prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b378dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install chromadb langchain-chroma langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ed2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings (using Google's embedding model)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d92687",
   "metadata": {},
   "source": [
    "## 1. Create Vector Store from Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36579ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI topics\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain is a framework for building LLM applications\", metadata={\"topic\": \"langchain\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"RAG stands for Retrieval-Augmented Generation. It combines retrieval with generation.\", metadata={\"topic\": \"rag\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"Vector databases store embeddings for semantic search\", metadata={\"topic\": \"vectordb\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"Agents can use tools to interact with external systems\", metadata={\"topic\": \"agents\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"Prompt engineering is the art of crafting effective prompts for LLMs\", metadata={\"topic\": \"prompts\", \"type\": \"definition\"}),\n",
    "    Document(page_content=\"Chroma is an open-source vector database that runs locally\", metadata={\"topic\": \"vectordb\", \"type\": \"tool\"}),\n",
    "    Document(page_content=\"Embeddings convert text into numerical vectors that capture semantic meaning\", metadata={\"topic\": \"embeddings\", \"type\": \"definition\"}),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8889ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store (in-memory)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"ai_knowledge\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cceab3",
   "metadata": {},
   "source": [
    "## 2. Similarity Search\n",
    "\n",
    "Find documents most similar to a query based on semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3544b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic similarity search\n",
    "query = \"What is RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "    print(f\"   Metadata: {doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search with scores (lower score = more similar)\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Score: {score:.4f} | {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ba800",
   "metadata": {},
   "source": [
    "## 3. Filtering with Metadata\n",
    "\n",
    "Filter search results based on document metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75edc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filter\n",
    "results = vectorstore.similarity_search(\n",
    "    \"database\",\n",
    "    k=3,\n",
    "    filter={\"topic\": \"vectordb\"}  # Only search in vectordb topic\n",
    ")\n",
    "\n",
    "print(\"Results filtered by topic='vectordb':\\n\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974cc48e",
   "metadata": {},
   "source": [
    "## 4. Persistent Vector Store\n",
    "\n",
    "Save the vector store to disk so it persists between sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82368938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent vector store (saves to disk)\n",
    "persist_directory = \"../data/chroma_db\"\n",
    "\n",
    "persistent_vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"ai_knowledge_persistent\",\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "print(f\"Vector store saved to: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing vector store from disk (in a new session)\n",
    "loaded_vectorstore = Chroma(\n",
    "    collection_name=\"ai_knowledge_persistent\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Test the loaded vector store\n",
    "results = loaded_vectorstore.similarity_search(\"What are AI agents?\", k=2)\n",
    "print(\"Results from loaded vector store:\")\n",
    "for doc in results:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d335de",
   "metadata": {},
   "source": [
    "## 5. Add & Delete Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1851b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new documents to existing vector store\n",
    "new_docs = [\n",
    "    Document(page_content=\"Fine-tuning adapts a pre-trained model to specific tasks\", metadata={\"topic\": \"training\"}),\n",
    "    Document(page_content=\"Temperature controls randomness in LLM outputs\", metadata={\"topic\": \"llm\"}),\n",
    "]\n",
    "\n",
    "# Add documents and get their IDs\n",
    "ids = vectorstore.add_documents(new_docs)\n",
    "print(f\"Added {len(ids)} documents with IDs: {ids}\")\n",
    "print(f\"Total documents now: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete documents by ID\n",
    "vectorstore.delete(ids=ids)\n",
    "print(f\"Deleted documents. Total now: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187046d9",
   "metadata": {},
   "source": [
    "## 6. Use as Retriever\n",
    "\n",
    "Convert the vector store to a retriever for use in RAG chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to retriever (for use in RAG chains)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for diversity\n",
    "    search_kwargs={\"k\": 3}     # return top 3 results\n",
    ")\n",
    "\n",
    "# Use the retriever\n",
    "docs = retriever.invoke(\"How do embeddings work?\")\n",
    "print(\"Retrieved documents:\")\n",
    "for doc in docs:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d33984",
   "metadata": {},
   "source": [
    "## 7. Load PDFs into Vector Store (Real-World Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "pdf_path = \"../data/AI ML Engineer & Agentic AI Engineer.pdf\"\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_docs = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "\n",
    "print(f\"Split PDF into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a678bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from PDF chunks\n",
    "pdf_vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"pdf_knowledge\"\n",
    ")\n",
    "\n",
    "# Search the PDF content\n",
    "query = \"What are the technical skills?\"\n",
    "results = pdf_vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25579cdf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Feature | Method |\n",
    "|---------|--------|\n",
    "| Create from docs | `Chroma.from_documents(docs, embedding)` |\n",
    "| Similarity search | `vectorstore.similarity_search(query, k=3)` |\n",
    "| Search with scores | `vectorstore.similarity_search_with_score(query)` |\n",
    "| Filter by metadata | `similarity_search(query, filter={\"key\": \"value\"})` |\n",
    "| Persist to disk | `Chroma(..., persist_directory=\"./path\")` |\n",
    "| Load from disk | `Chroma(persist_directory=\"./path\", embedding_function=emb)` |\n",
    "| Add documents | `vectorstore.add_documents(docs)` |\n",
    "| Delete documents | `vectorstore.delete(ids=[...])` |\n",
    "| Convert to retriever | `vectorstore.as_retriever()` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentic AI (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
